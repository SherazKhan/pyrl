"""
Economic choice task, based on

  Neurons in the orbitofrontal cortex encode economic value.
  C. Padoa-Schioppa & J. A. Assad, Nature 2006.

  http://dx.doi.org/10.1038/nature04676

"""
from __future__ import division

import numpy as np

from pyrl import tasktools

# Inputs
inputs = tasktools.to_map('FIXATION', 'L-A', 'L-B', 'R-A', 'R-B', 'N-L', 'N-R')

# Actions
actions = tasktools.to_map('FIXATE', 'CHOOSE-LEFT', 'CHOOSE-RIGHT')

# Trial conditions
A_to_B       = 2
juices       = [('A', 'B'), ('B', 'A')]
offers       = [(0, 1), (1, 3), (1, 2), (1, 1), (2, 1), (3, 1), (4, 1), (6, 1), (2, 0)]
n_conditions = len(juices) * len(offers)

# Training
n_gradient   = 2*n_conditions
n_validation = 50*n_conditions

# Durations
fixation     = 750 # 1500
offer_on_min = 1000
offer_on_max = 2000
choice_init  = 1500
choice_hold  = 50
decision     = choice_init + choice_hold
tmax         = fixation + offer_on_max + decision

# Rewards
R_ABORTED = -1
R_B       = 0.2
R_A       = A_to_B * R_B

# Input scaling
def scale(x):
    return x/5

class Task(object):
    def start_trial(self):
        self.a_choice = None
        self.t_choice = None

    def get_condition(self, rng, dt, context={}):
        #-------------------------------------------------------------------------------------
        # Epochs
        #-------------------------------------------------------------------------------------

        offer_on = context.get('offer-on')
        if offer_on is None:
            offer_on = tasktools.uniform(rng, dt, offer_on_min, offer_on_max)

        durations = {
            'fixation':    (0, fixation),
            'offer-on':    (fixation, fixation + offer_on),
            'choice-init': (fixation + offer_on,
                            fixation + offer_on + choice_init),
            'decision':    (fixation + offer_on, tmax),
            'tmax':        tmax
            }
        time, epochs = tasktools.get_epochs_idx(dt, durations)

        #-------------------------------------------------------------------------------------
        # Trial
        #-------------------------------------------------------------------------------------

        juice = context.get('juice')
        if juice is None:
            juice = tasktools.choice(rng, juices)

        offer = context.get('offer')
        if offer is None:
            offer = tasktools.choice(rng, offers)

        juiceL, juiceR = juice
        nB, nA = offer

        if juiceL == 'A':
            nL, nR = nA, nB
        else:
            nL, nR = nB, nA

        return {
            'durations': durations,
            'time':      time,
            'epochs':    epochs,
            'juice':     juice,
            'offer':     offer,
            'nL':        nL,
            'nR':        nR
            }

    def get_step(self, rng, dt, trial, t, a):
        #-------------------------------------------------------------------------------------
        # Reward
        #-------------------------------------------------------------------------------------

        time   = trial['time']
        epochs = trial['epochs']
        status = {'continue': True}
        reward = 0

        if t-1 in epochs['fixation'] or t-1 in epochs['offer-on']:
            if a != actions['FIXATE']:
                status['continue'] = False
                reward = R_ABORTED
        elif t-1 in epochs['decision']:
            if self.a_choice is None:
                if t-1 not in epochs['choice-init']:
                    status['continue'] = False
                    reward = R_ABORTED
                elif a in [actions['CHOOSE-LEFT'], actions['CHOOSE-RIGHT']]:
                    self.a_choice = a
                    self.t_choice = time[t-1]
            else:
                if a != actions['FIXATE']:
                    status['continue'] = False
                    reward = R_ABORTED
                    #held_time = time[t-1] - self.t_choice
                    #reward = np.exp(-(time[t-1] - self.t_choice)/500) * R_ABORTED
                    #print("{} -> {} {} {}".format(self.choice, a, held_time, reward))
                else:
                    if time[t-1] - self.t_choice >= choice_hold:
                        status['continue'] = False
                        status['t_choice'] = self.t_choice

                        #-----------------------------------------------------------------
                        # Collect reward
                        #-----------------------------------------------------------------

                        juiceL, juiceR = trial['juice']
                        nB, nA = trial['offer']

                        rA = nA * R_A
                        rB = nB * R_B

                        if juiceL == 'A':
                            rL, rR = rA, rB
                        else:
                            rL, rR = rB, rA

                        if self.a_choice == actions['CHOOSE-LEFT']:
                            if juiceL == 'A':
                                status['choice'] = 'A'
                            else:
                                status['choice'] = 'B'
                            status['correct'] = (rL >= rR)
                            reward = rL
                        elif self.a_choice == actions['CHOOSE-RIGHT']:
                            if juiceR == 'A':
                                status['choice'] = 'A'
                            else:
                                status['choice'] = 'B'
                            status['correct'] = (rR >= rL)
                            reward = rR

        #-------------------------------------------------------------------------------------
        # Inputs
        #-------------------------------------------------------------------------------------

        u = np.zeros(len(inputs))
        if t not in epochs['decision']:
            u[inputs['FIXATION']] = 1
        if t in epochs['offer-on']:
            juiceL, juiceR = trial['juice']
            u[inputs['L-'+juiceL]] = 1
            u[inputs['R-'+juiceR]] = 1

            u[inputs['N-L']] = scale(trial['nL'])
            u[inputs['N-R']] = scale(trial['nR'])

        #-------------------------------------------------------------------------------------

        return u, reward, status

    def terminate(self, perf):
        p_decision, p_correct = tasktools.correct_2AFC(perf)

        return p_decision >= 0.99 and p_correct >= 0.95
